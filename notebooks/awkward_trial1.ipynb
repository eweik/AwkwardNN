{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AwkwardNN: Trial 1\n",
    "\n",
    "### Get awkward toy data\n",
    "(assume awkward is installed)\n",
    "\n",
    "- Focus on just nested and jagged arrays\n",
    "- Assign maximum possible length and max possible depth for all arrays.\n",
    "    * Randomly pick max length/depth from range [1, max] for each event\n",
    "    and max length again for each sublist/nested list.\n",
    "- Binary targets: 0, 1\n",
    "- Data drawn from\n",
    "    * Gaussian(-L, 1) for target 0, where L is level of data\n",
    "        * e.g. for the highest level of data (nest 0) with max depth 5,\n",
    "        data would be drawn from Gaussian(-5, 1), for data within the\n",
    "        first nest, the max depth is 4 so data would be drawn from\n",
    "        Gaussian(-4, 1), and so on.\n",
    "    * Gaussian(+L, 1) for target 1\n",
    "- Assign probabilities for possible elements in array. Example:\n",
    "    * p( signal / target 1 ) = 0.30\n",
    "    * p( noise / target 0 ) = 0.30\n",
    "    * p( subarray ) = 0.40\n",
    "- Starting with empty list, randomly sample elements until max length\n",
    "is reached. When max depth is reached, only sample from p(signal)\n",
    "and p(noise)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: [[4.429801596478066] [1.550401341203096] [3.722566914905654 1.336849830707411 3.3895145610720516]]\n",
      "1: [4.77513166]\n",
      "1: [[2.091071741655326] [2.930729509983566 [2.341295749404614 1.074742178966952 [0.7417639830926431 1.674351306604812] 1.5545883223375734] 1.8451455792507119] 5.106839460671908]\n",
      "0: [-3.74539092]\n",
      "1: [5.290886329859674 [1.702121814823695 3.175989892509131 2.736146492136837]]\n"
     ]
    }
   ],
   "source": [
    "import awkward\n",
    "from awkwardNN.createAwkwardData import generate_data_target\n",
    "\n",
    "max_len = 4\n",
    "max_depth = 4\n",
    "p_signal = 0.30\n",
    "p_noise = 0.30\n",
    "p_subarray = 0.40\n",
    "num_events = 10000\n",
    "\n",
    "X_train, y_train = generate_data_target(num_events=num_events,\n",
    "                                        prob_nest=p_subarray,\n",
    "                                        prob_sig=p_signal,\n",
    "                                        prob_bkg=p_noise,\n",
    "                                        max_len=max_len,\n",
    "                                        max_depth=max_depth)\n",
    "\n",
    "X_test, y_test = generate_data_target(num_events=1000,\n",
    "                                      prob_nest=p_subarray,\n",
    "                                      prob_sig=p_signal,\n",
    "                                      prob_bkg=p_noise,\n",
    "                                      max_len=max_len,\n",
    "                                      max_depth=max_depth)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"{}: {}\".format(y_train[i], X_train[i]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Pytorch dataloader for neural net"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "from awkwardNN.preprocessAwkwardData import AwkwardDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainset = AwkwardDataset(X_train, y_train)\n",
    "trainsize = len(trainset)\n",
    "trainloader = DataLoader(trainset, batch_size=1, shuffle=True)\n",
    "\n",
    "testset = AwkwardDataset(X_test, y_test)\n",
    "testsize = len(testset)\n",
    "testloader = DataLoader(testset, batch_size=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create pytorch neural net\n",
    "\n",
    "First trial network: flattening all the data from an event into one list and passing through an RNN.\n",
    "Loses information from nesting structure.\n",
    "\n",
    "Second trial network:\n",
    "- Recurrent neural network with a layer for each nested level of data.\n",
    "- RNN processes an event one nested level at a time, passing each level through its associated layer in order from highest level to lowest level.\n",
    "- Append hidden state to input for each layer, then use hidden state for through final output layer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class AwkwardNeuralNet(nn.Module):\n",
    "    def __init__(self, max_depth, input_sz, hidden_sz, output_sz):\n",
    "        super(AwkwardNeuralNet, self).__init__()\n",
    "        self.max_depth = max_depth\n",
    "        self.input_sz = input_sz\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.layers = []\n",
    "        for _ in range(max_depth):\n",
    "            self.layers.append( nn.Linear(input_sz + hidden_sz, hidden_sz) )\n",
    "        self.output = nn.Linear(hidden_sz, output_sz)\n",
    "\n",
    "    def forward(self, input_data, markers, hidden):\n",
    "        i = 0\n",
    "        # since we're not iterating over batches\n",
    "        input_data, markers = input_data[0], markers[0]\n",
    "        for marker, net_layer in zip(markers, self.layers):\n",
    "            if marker == 0:\n",
    "                continue\n",
    "            for _ in range(marker):\n",
    "                x = torch.tensor([[input_data[i]]], dtype=torch.float32)\n",
    "                combined = torch.cat((x, hidden), 1)\n",
    "                hidden = F.relu(net_layer(combined))\n",
    "                i += 1\n",
    "        output = F.log_softmax(self.output(hidden), dim=1)\n",
    "        return output, hidden"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train neural net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/5\n",
      "Train Epoch: 1 [1000/10000 (10%)]\tLoss: 0.755006\tAcc: 0%\n",
      "Train Epoch: 1 [2000/10000 (20%)]\tLoss: 0.285595\tAcc: 100%\n",
      "Train Epoch: 1 [3000/10000 (30%)]\tLoss: 0.063075\tAcc: 100%\n",
      "Train Epoch: 1 [4000/10000 (40%)]\tLoss: 0.028615\tAcc: 100%\n",
      "Train Epoch: 1 [5000/10000 (50%)]\tLoss: 0.246420\tAcc: 100%\n",
      "Train Epoch: 1 [6000/10000 (60%)]\tLoss: 0.631293\tAcc: 100%\n",
      "Train Epoch: 1 [7000/10000 (70%)]\tLoss: 0.025031\tAcc: 100%\n",
      "Train Epoch: 1 [8000/10000 (80%)]\tLoss: 0.006828\tAcc: 100%\n",
      "Train Epoch: 1 [9000/10000 (90%)]\tLoss: 0.103020\tAcc: 100%\n",
      "Train Epoch: 1 [10000/10000 (100%)]\tLoss: 0.153447\tAcc: 100%\n",
      "\n",
      "Epoch: 2/5\n",
      "Train Epoch: 2 [1000/10000 (10%)]\tLoss: 0.022395\tAcc: 100%\n",
      "Train Epoch: 2 [2000/10000 (20%)]\tLoss: 0.369395\tAcc: 100%\n",
      "Train Epoch: 2 [3000/10000 (30%)]\tLoss: 0.097598\tAcc: 100%\n",
      "Train Epoch: 2 [4000/10000 (40%)]\tLoss: 0.000138\tAcc: 100%\n",
      "Train Epoch: 2 [5000/10000 (50%)]\tLoss: 0.013478\tAcc: 100%\n",
      "Train Epoch: 2 [6000/10000 (60%)]\tLoss: 0.010273\tAcc: 100%\n",
      "Train Epoch: 2 [7000/10000 (70%)]\tLoss: 0.121556\tAcc: 100%\n",
      "Train Epoch: 2 [8000/10000 (80%)]\tLoss: 0.002989\tAcc: 100%\n",
      "Train Epoch: 2 [9000/10000 (90%)]\tLoss: 0.058414\tAcc: 100%\n",
      "Train Epoch: 2 [10000/10000 (100%)]\tLoss: 0.020466\tAcc: 100%\n",
      "\n",
      "Epoch: 3/5\n",
      "Train Epoch: 3 [1000/10000 (10%)]\tLoss: 0.000442\tAcc: 100%\n",
      "Train Epoch: 3 [2000/10000 (20%)]\tLoss: 0.001633\tAcc: 100%\n",
      "Train Epoch: 3 [3000/10000 (30%)]\tLoss: 0.001546\tAcc: 100%\n",
      "Train Epoch: 3 [4000/10000 (40%)]\tLoss: 0.000480\tAcc: 100%\n",
      "Train Epoch: 3 [5000/10000 (50%)]\tLoss: 0.146249\tAcc: 100%\n",
      "Train Epoch: 3 [6000/10000 (60%)]\tLoss: 0.000003\tAcc: 100%\n",
      "Train Epoch: 3 [7000/10000 (70%)]\tLoss: 0.036877\tAcc: 100%\n",
      "Train Epoch: 3 [8000/10000 (80%)]\tLoss: 0.002543\tAcc: 100%\n",
      "Train Epoch: 3 [9000/10000 (90%)]\tLoss: 0.000199\tAcc: 100%\n",
      "Train Epoch: 3 [10000/10000 (100%)]\tLoss: 0.000019\tAcc: 100%\n",
      "\n",
      "Epoch: 4/5\n",
      "Train Epoch: 4 [1000/10000 (10%)]\tLoss: 0.071591\tAcc: 100%\n",
      "Train Epoch: 4 [2000/10000 (20%)]\tLoss: 0.034454\tAcc: 100%\n",
      "Train Epoch: 4 [3000/10000 (30%)]\tLoss: 0.010710\tAcc: 100%\n",
      "Train Epoch: 4 [4000/10000 (40%)]\tLoss: 0.002490\tAcc: 100%\n",
      "Train Epoch: 4 [5000/10000 (50%)]\tLoss: 0.010849\tAcc: 100%\n",
      "Train Epoch: 4 [6000/10000 (60%)]\tLoss: 0.342755\tAcc: 100%\n",
      "Train Epoch: 4 [7000/10000 (70%)]\tLoss: 0.003161\tAcc: 100%\n",
      "Train Epoch: 4 [8000/10000 (80%)]\tLoss: 0.000001\tAcc: 100%\n",
      "Train Epoch: 4 [9000/10000 (90%)]\tLoss: 0.009291\tAcc: 100%\n",
      "Train Epoch: 4 [10000/10000 (100%)]\tLoss: 0.013261\tAcc: 100%\n",
      "\n",
      "Epoch: 5/5\n",
      "Train Epoch: 5 [1000/10000 (10%)]\tLoss: 0.155227\tAcc: 100%\n",
      "Train Epoch: 5 [2000/10000 (20%)]\tLoss: 0.064547\tAcc: 100%\n",
      "Train Epoch: 5 [3000/10000 (30%)]\tLoss: 0.000019\tAcc: 100%\n",
      "Train Epoch: 5 [4000/10000 (40%)]\tLoss: 0.000041\tAcc: 100%\n",
      "Train Epoch: 5 [5000/10000 (50%)]\tLoss: 0.160192\tAcc: 100%\n",
      "Train Epoch: 5 [6000/10000 (60%)]\tLoss: 0.000011\tAcc: 100%\n",
      "Train Epoch: 5 [7000/10000 (70%)]\tLoss: 0.000332\tAcc: 100%\n",
      "Train Epoch: 5 [8000/10000 (80%)]\tLoss: 0.000468\tAcc: 100%\n",
      "Train Epoch: 5 [9000/10000 (90%)]\tLoss: 0.000001\tAcc: 100%\n",
      "Train Epoch: 5 [10000/10000 (100%)]\tLoss: 0.010846\tAcc: 100%\n",
      "\n",
      "[*] Test set:\n",
      "    Avg. loss: 0.0878, Accuracy: 9664/10000 (97%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.08776980313313719, tensor(96.6400))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import awkwardNN.utils as utils\n",
    "\n",
    "hidden_size = 32\n",
    "in_size = 1\n",
    "out_size = trainset.get_output_size()\n",
    "max_depth = trainset.get_max_depth()\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "batch_size = 1\n",
    "print_freq = int(num_events / 10)\n",
    "\n",
    "model = AwkwardNeuralNet(max_depth=max_depth,\n",
    "                         input_sz=in_size,\n",
    "                         hidden_sz=hidden_size,\n",
    "                         output_sz=out_size)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        print('\\nEpoch: {}/{}'.format(epoch+1, epochs))\n",
    "        train_one_epoch(epoch)\n",
    "    return\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    losses, accs = utils.AverageMeter(), utils.AverageMeter()\n",
    "    model.train()\n",
    "    for i, (x, marker, y) in enumerate(trainloader):\n",
    "        x, marker, y, hidden = _reset(x, marker, y, batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            y_hat, hidden = model(x, marker, hidden)\n",
    "            loss = utils.get_loss(y, y_hat)\n",
    "            acc = utils.get_accuracy(y, y_hat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.update(loss.item(), x.size(0))\n",
    "            accs.update(acc.item(), x.size(0))\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            utils.print_train_stat(epoch+1, i+print_freq, x, num_events, loss, acc)\n",
    "    return losses.avg, accs.avg\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    losses = utils.AverageMeter()\n",
    "    model.eval()\n",
    "    for i, (x, marker, y) in enumerate(testloader):\n",
    "        x, marker, y, hidden = _reset(x, marker, y, batch_size)\n",
    "        with torch.no_grad():\n",
    "            y_hat, _ = model(x, marker, hidden)\n",
    "            loss = utils.get_loss(y, y_hat)\n",
    "            _, prediction = torch.max(y_hat, 1)\n",
    "            correct += prediction.eq(y.data.view_as(prediction)).sum()\n",
    "            losses.update(loss.item(), x.size(0))\n",
    "    acc = 100. * correct / testsize\n",
    "    utils.print_test_set(losses.avg, correct, acc, testsize)\n",
    "    return losses.avg, acc\n",
    "\n",
    "\n",
    "def _reset(x, marker, y, batch_size):\n",
    "    x = x.to(device)\n",
    "    marker = marker.to(device)\n",
    "    y = y.to(device)\n",
    "    hidden = torch.zeros(batch_size, hidden_size)\n",
    "    hidden = hidden.to(device)\n",
    "    return x, marker, y, hidden\n",
    "\n",
    "train()\n",
    "test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Current issues/problems:\n",
    "\n",
    "- Pytorch only takes in `Tensor` objects, which have to be rectangular like numpy arrays\n",
    "- Varying length arrays → hard to batch data\n",
    "    - because `Tensor` objects have to be rectangular\n",
    "    - Could zero pad and then ignore those zeros\n",
    "    - Currently: go through data one event at a time\n",
    "- Other ways to pass an Awkward Data Structure through an RNN are possible, perhaps\n",
    "consider other architectures"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}