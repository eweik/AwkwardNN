{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# AwkwardNN: Trial 2\n",
    "\n",
    "### Get ROOT Data\n",
    "\n",
    "First naive approach:\n",
    "- The data is stored in column/feature format on ROOT files\n",
    "- But we want to pass forward row/event formatted data through\n",
    "the neural net\n",
    "- So, beforehand, I convert from column to row based data as I\n",
    "read it in.\n",
    "    - But, this is kind of slow since I have to iterate\n",
    "through each row in each column. Currently, awkward-array does\n",
    "not support this type of operation in a vectorized manner.\n",
    "- Result: a list of events, each with varying numbers of particles\n",
    "and where each particle has varying numbers of features.\n",
    "- Note: current `uproot` memory issue reading in features:\n",
    "    - b'Particle.fBits'\n",
    "    - b'Track.fBits'\n",
    "    - b'Tower.fBits'\n",
    "    - b'EFlowTrack.fBits'\n",
    "    - b'EFlowPhoton.fBits'\n",
    "    - b'EFlowNeutralHadron.fBits'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edisonweik/.pyenv/versions/3.6.10/lib/python3.6/site-packages/awkward/array/base.py:394: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return cls.numpy.array(value, copy=False)\n"
     ]
    }
   ],
   "source": [
    "import uproot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from awkwardNN.awkwardNN import awkwardNN\n",
    "from awkwardNN.preprocessRoot import get_events\n",
    "\n",
    "tree1 = uproot.open(\"../data/test_qcd_1000.root\")[\"Delphes\"]\n",
    "tree2 = uproot.open(\"../data/test_ttbar_1000.root\")[\"Delphes\"]\n",
    "\n",
    "# Can choose fields based on names for which to train on\n",
    "fields = [\"Jet*\"]\n",
    "X1 = get_events(tree1, fields)\n",
    "X2 = get_events(tree2, fields)\n",
    "y1 = [1] * len(X1)\n",
    "y2 = [0] * len(X2)\n",
    "X = X1 + X2\n",
    "y = y1 + y2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create and train pytorch neural net\n",
    "\n",
    "Training procedure:\n",
    "- For each event:\n",
    "    - initialize first hidden state\n",
    "    - For each particle\n",
    "        - initialize second hidden state\n",
    "        - For each feature in particle\n",
    "            - pass feature + hidden states through RNN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid set (epoch 1:\n",
      "    Avg. loss: 0.6935, Accuracy: 93/180 (52%) [*]\n",
      "\n",
      "Valid set (epoch 2:\n",
      "    Avg. loss: 0.6931, Accuracy: 92/180 (51%)\n",
      "\n",
      "Valid set (epoch 3:\n",
      "    Avg. loss: 0.6919, Accuracy: 102/180 (57%) [*]\n",
      "\n",
      "Valid set (epoch 4:\n",
      "    Avg. loss: 0.6921, Accuracy: 106/180 (59%) [*]\n",
      "\n",
      "Valid set (epoch 5:\n",
      "    Avg. loss: 0.6934, Accuracy: 86/180 (48%)\n",
      "\n",
      "Valid set (epoch 6:\n",
      "    Avg. loss: 0.6941, Accuracy: 86/180 (48%)\n",
      "\n",
      "Valid set (epoch 7:\n",
      "    Avg. loss: 0.6933, Accuracy: 86/180 (48%)\n",
      "\n",
      "Valid set (epoch 8:\n",
      "    Avg. loss: 0.6940, Accuracy: 86/180 (48%)\n",
      "\n",
      "Valid set (epoch 9:\n",
      "    Avg. loss: 0.6939, Accuracy: 86/180 (48%)\n",
      "\n",
      "Valid set (epoch 10:\n",
      "    Avg. loss: 0.6939, Accuracy: 86/180 (48%)\n",
      "\n",
      "[*] Test set:\n",
      "    Avg. loss: 0.6925, Accuracy: 107/200 (54%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0.6924584510422283, tensor(53.5000))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from awkwardNN.awkwardNN import awkwardNN\n",
    "\n",
    "# Trial for 10 epochs\n",
    "# other arguments possible to set for Awkward NN initialization\n",
    "# can specify hidden layers sizes, number of layers, etc.\n",
    "model1 = awkwardNN(mode='rnn', max_iter=10)\n",
    "#model1 = awkwardNN(mode='lstm', max_iter=10)\n",
    "#model1 = awkwardNN(mode='gru', max_iter=10)\n",
    "\n",
    "model1.train(X_train, y_train)\n",
    "model1.test(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Can also make DeepSet networks\n",
    "\n",
    "Model:\n",
    "- 2 Deepsets\n",
    "    - one for particles in events\n",
    "    - one for features in particles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid set (epoch 1:\n",
      "    Avg. loss: 934.4925, Accuracy: 102/180 (57%) [*]\n",
      "\n",
      "Valid set (epoch 2:\n",
      "    Avg. loss: 312.9083, Accuracy: 77/180 (43%)\n",
      "\n",
      "Valid set (epoch 3:\n",
      "    Avg. loss: 228.3764, Accuracy: 77/180 (43%)\n",
      "\n",
      "Valid set (epoch 4:\n",
      "    Avg. loss: 1076.7963, Accuracy: 102/180 (57%)\n",
      "\n",
      "Valid set (epoch 5:\n",
      "    Avg. loss: 222.8156, Accuracy: 77/180 (43%)\n",
      "\n",
      "Valid set (epoch 6:\n",
      "    Avg. loss: 262.3893, Accuracy: 102/180 (57%)\n",
      "\n",
      "Valid set (epoch 7:\n",
      "    Avg. loss: 220.2862, Accuracy: 102/180 (57%)\n",
      "\n",
      "Valid set (epoch 8:\n",
      "    Avg. loss: 439.0076, Accuracy: 102/180 (57%)\n",
      "\n",
      "Valid set (epoch 9:\n",
      "    Avg. loss: 17.4613, Accuracy: 102/180 (57%)\n",
      "\n",
      "Valid set (epoch 10:\n",
      "    Avg. loss: 532.0515, Accuracy: 102/180 (57%)\n",
      "\n",
      "[*] Test set:\n",
      "    Avg. loss: 1053.0035, Accuracy: 107/200 (54%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "(1053.0034724855886, tensor(53.5000))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = awkwardNN(mode='deepset', max_iter=10)\n",
    "model2.train(X_train, y_train)\n",
    "model2.test(X_test, y_test)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}